{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *arg\n",
    "\n",
    "We use this for when the number of input args is unknown\n",
    "\n",
    "\n",
    "Same thing can be used when there are multiple unknown key word args, we can use **kwargs\n",
    "\n",
    "(by convention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "intersection_among_lists = lambda x, y: x | y\n",
    "\n",
    "ls1 = [2, 3, 4]\n",
    "ls2= [3, 4, 5]\n",
    "ls3 = [3]\n",
    "\n",
    "def common_elements(*arg):\n",
    "    result = set(arg[0])\n",
    "    for i in range(1, len(arg)):\n",
    "        result = result & set(arg[i])\n",
    "        \n",
    "    return list(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_elements(ls1, ls2, ls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using map-reducing\n",
    "\n",
    "def common_elements(*args):\n",
    "    # convert each list to a set\n",
    "    args = map(lambda x: set(x), args)  # can also just pass in set, because it's already a function (statement)\n",
    "    # reduce all the lists to a single set\n",
    "    args = reduce(lambda x, y: x & y, args)\n",
    "    return list(args)\n",
    "\n",
    "common_elements(ls1, ls2, ls3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Why is Map Reduce Better?\n",
    "\n",
    "Because when we have lots of computational resources at our disposal, the map reduce functions make the most efficient use of them - because they can distribute the processing power they use across multiple CPU cores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark installation on local machine\n",
    "\n",
    "Pyspark is basically the Python SDK for Spark, the most popular Big Data platform (written in Java and Scala)\n",
    "\n",
    "Pyspark on local machine is not the best platform though, according to Milad - use AWS, or databricks even more\n",
    "\n",
    "Pyspark on local machine is not so useful, you cannot have clusters but it is for learning\n",
    "\n",
    "1. Install jdk 8 or higher via https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "2. install pyspark with `brew install apache-spark`\n",
    "3. install pyspark and findspark via \n",
    "    - `pip install findspark`\n",
    "    - `pip install pyspark`\n",
    "4. In either your bashrc, zshrc, or whatever shell config you use, enter this:\n",
    "    * export PYSPARK_PYTHON=python3\n",
    "    * export SPARK_HOME=/usr/local/lib/python3.7/site-packages/pyspark\n",
    "5. In your text editor with a python file open, type the following:\n",
    "\n",
    "\n",
    "\n",
    "Why we use Pyspark\n",
    "\n",
    "1. Big Data array manipulation\n",
    "2. Big dataframe manipulation\n",
    "3. Train Ml models on Big Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Python Spark regression example\") \\\n",
    "   .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/downloads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|radio|newspaper|sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "|  8.7| 48.9|     75.0|  7.2|\n",
      "| 57.5| 32.8|     23.5| 11.8|\n",
      "|120.2| 19.6|     11.6| 13.2|\n",
      "|  8.6|  2.1|      1.0|  4.8|\n",
      "|199.8|  2.6|     21.2| 10.6|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD as lrSGD\n",
    "\n",
    "\n",
    "regressionDataFrame = spark.read.csv('Advertising.csv',header=True, inferSchema = True)\n",
    "# drop the headers\n",
    "regressionDataFrame = regressionDataFrame.drop('_c0')\n",
    "# show the top 10 rows\n",
    "regressionDataFrame.show(10)\n",
    "# making an RDD\n",
    "regressionDataRDD = regressionDataFrame.rdd.map(list)\n",
    "# deciding the predictor and target variables?\n",
    "regressionDataLabelPoint = regressionDataRDD.map(lambda data : LabeledPoint(data[3], data[0:3]))\n",
    "# data splitting\n",
    "regressionLabelPointSplit = regressionDataLabelPoint.randomSplit([0.7, 0.3])\n",
    "\n",
    "regressionLabelPointTrainData = regressionLabelPointSplit[0]\n",
    "\n",
    "regressionLabelPointTestData = regressionLabelPointSplit[1]\n",
    "\n",
    "# training the model\n",
    "ourModelWithLinearRegression  = lrSGD.train(data = regressionLabelPointTrainData, \n",
    "                                            iterations = 200, step = 0.02, intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds23_env",
   "language": "python",
   "name": "ds23_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
