{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *arg\n",
    "\n",
    "We use this for when the number of input args is unknown\n",
    "\n",
    "\n",
    "Same thing can be used when there are multiple unknown key word args, we can use **kwargs\n",
    "\n",
    "(by convention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "intersection_among_lists = lambda x, y: x | y\n",
    "\n",
    "ls1 = [2, 3, 4]\n",
    "ls2= [3, 4, 5]\n",
    "ls3 = [3]\n",
    "\n",
    "def common_elements(*arg):\n",
    "    result = set(arg[0])\n",
    "    for i in range(1, len(arg)):\n",
    "        result = result & set(arg[i])\n",
    "        \n",
    "    return list(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_elements(ls1, ls2, ls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using map-reducing\n",
    "\n",
    "def common_elements(*args):\n",
    "    # convert each list to a set\n",
    "    args = map(lambda x: set(x), args)  # can also just pass in set, because it's already a function (statement)\n",
    "    # reduce all the lists to a single set\n",
    "    args = reduce(lambda x, y: x & y, args)\n",
    "    return list(args)\n",
    "\n",
    "common_elements(ls1, ls2, ls3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Why is Map Reduce Better?\n",
    "\n",
    "Because when we have lots of computational resources at our disposal, the map reduce functions make the most efficient use of them - because they can distribute the processing power they use across multiple CPU cores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark installation on local machine\n",
    "\n",
    "Pyspark is basically the Python SDK for Spark, the most popular Big Data platform (written in Java and Scala)\n",
    "\n",
    "Pyspark on local machine is not the best platform though, according to Milad - use AWS, or databricks even more\n",
    "\n",
    "Pyspark on local machine is not so useful, you cannot have clusters but it is for learning\n",
    "\n",
    "1. Install jdk 8 or higher via https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "2. install pyspark with `brew install apache-spark`\n",
    "3. install pyspark and findspark via \n",
    "    - `pip install findspark`\n",
    "    - `pip install pyspark`\n",
    "4. In either your bashrc, zshrc, or whatever shell config you use, enter this:\n",
    "    * export PYSPARK_PYTHON=python3\n",
    "    * export SPARK_HOME=/usr/local/lib/python3.7/site-packages/pyspark\n",
    "5. In your text editor with a python file open, type the following:\n",
    "\n",
    "\n",
    "\n",
    "Why we use Pyspark\n",
    "\n",
    "1. Big Data array manipulation\n",
    "2. Big dataframe manipulation\n",
    "3. Train Ml models on Big Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Python Spark regression example\") \\\n",
    "   .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/downloads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-4-110e88b7089a>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee18f58143ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegressionWithSGD\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlrSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/dev/courses/Year_2/DS/DS-2.3-Data-Science-in-Production-master/env/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/Downloads/dev/courses/Year_2/DS/DS-2.3-Data-Science-in-Production-master/env/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-4-110e88b7089a>:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD as lrSGD\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark regression example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "regressionDataFrame = spark.read.csv('Advertising.csv',header=True, inferSchema = True)\n",
    "\n",
    "regressionDataFrame = regressionDataFrame.drop('_c0')\n",
    "\n",
    "regressionDataFrame.show(10)\n",
    "\n",
    "regressionDataRDD = regressionDataFrame.rdd.map(list)\n",
    "\n",
    "\n",
    "regressionDataLabelPoint = regressionDataRDD.map(lambda data : LabeledPoint(data[3], data[0:3]))\n",
    "\n",
    "regressionLabelPointSplit = regressionDataLabelPoint.randomSplit([0.7, 0.3])\n",
    "\n",
    "regressionLabelPointTrainData = regressionLabelPointSplit[0]\n",
    "\n",
    "regressionLabelPointTestData = regressionLabelPointSplit[1]\n",
    "\n",
    "\n",
    "ourModelWithLinearRegression  = lrSGD.train(data = regressionLabelPointTrainData, \n",
    "                                            iterations = 200, step = 0.02, intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
